---
layout: post
title:  "Функция SoftMax"
date:   2018-10-26 20:00:00 +0500
categories: deep_learning
comments: true
math: true
---
Функция SoftMax, по-русски функция мягкого максимума, часто используется в нейронных сетях в качестве функции активации при решении задачи классификации. SoftMax задается следующей формулой:

$$\sigma(z)_i = \frac{e^{z_i}}{\sum_{k=1}^N e^{z_k}}$$

где $z_i$ -- значение на выходе из *i-го* нейрона до активации, а *N* -- общее количество нейронов в слое.

Почему именно эта функция используется для задач классификации?

<!--more-->

## Требования к функции активации для классификации

Давайте сформулируем, какой мы хотим видеть функцию активации для решения задачи классификации. Мы будем рассматривать задачу, в которой объект принадлежит только одному классу. Если классы могут пересекаться, то функция мягкого максимума не подходит. Требования выглядят следующим образом:

1. Значения на выходе из слоя могут трактоваться как вероятность принадлежности объекта заданному классу. Это означает, что каждое значение должно быть в диапазоне от 0 до 1, и сумма всех значений должна равняться единице.
2. Функция должна быть дифференцируема, чтобы можно было применять градиентные методы обучения нейронной сети.

## Возможные варианты функции активации для классификации

**1. Жесткий максимум**. На первый взгляд, это самая подходящая функция. Если у нас 5 нейронов в слое, и на выходе до активации мы имеем значения `[2, 4, 2, 1, 1]`, то после активации будет `[0, 1, 0, 0, 0]`. Нет возможности перепутать классы, единица только в одной позиции. Также удобно то, что правильные ответы у нас обычно именно в таком виде. К сожалению, функция жесткого максимума не дифференцируема, поэтому ее нельзя использовать для обучения нейронных сетей градиентными методами.

**2. Нормализация**. Можно попробовать использовать простую нормализацию, без экспоненты: 

$$z_i = \frac{z_i}{\sum_{k=1}^N z_k}$$

В результате сумма значений выходов нейронов будет равна единице, и такая функция дифференцируема. Для нашего примера выходных данных нейронов `[2, 4, 2, 1, 1]` после активации получится `[0.2, 0.4, 0.2, 0.1, 0.1]`. Однако здесь максимальное значение не так сильно отличается от остальных, что не очень эффективно для обучения сети. Хотелось бы иметь после активации значения ближе к `[0, 1, 0, 0, 0]`.

Еще один недостаток простой нормализации в том, что если на выходе из нейрона отрицательное значение, то и после активации значение будет отрицательным. Такое значение нельзя трактовать как вероятность принадлежности классу.

**3. Функция мягкого максимума**. Достоинство функции SoftMax в том, что она использует экспоненту. За счет этого решается проблема с отрицательными выходными значениями, т.к. экспонента всегда положительна. Кроме того, экспонента значительно увеличивает большие значения. Если на выходе из нейрона 1, то экспонента равна 2.7, если 2 -- экспонента равна 4.7, а если на выходе 4, то экспонента получается 54.6.

Для нашего примера из пяти нейронов после применения функции активации мягкого максимума получится вектор `[0.099, 0.730, 0.099, 0.036, 0.036]`. Как нам и хотелось, к единице близко только значение на выходе из второго нейрона, остальные значения близки к нулю. Кроме того, эта функция дифференцируема, значит не будет проблем с обучением.

## Итоги

Функцию активации мягкого максимума удобно применять для задач классификации, т.к. она позволяет трактовать выходные значения нейронов как вероятность принадлежности данному классу, а также обеспечивает, чтобы только одно выходное значение было близко к единице за счет применения экспоненты. 

Следует отметить, что нет теоретической необходимости использовать именно SoftMax для задач классификации. При наличии достаточного объема данных нейронная сеть может обучиться с другими функциями активации на последнем слое. Но с функцией мягкого максимума обучение произойдет быстрее, потому что данная функция хорошо подходит именно для классификации с непересекающимися классами.

## Полезные ссылки

1. [Функция SoftMax](https://ru.wikipedia.org/wiki/Softmax).
1. [In softmax classifier, why use exp function to do normalization?](https://datascience.stackexchange.com/questions/23159/in-softmax-classifier-why-use-exp-function-to-do-normalization).

